{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add your LiU-ID here:**\n",
    "* <liuid 1>\n",
    "* <liuid 2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **0. Quick introduction to jupyter notebooks**\n",
    "* Each cell in this notebook contains either code or text.\n",
    "* You can run a cell by pressing Ctrl-Enter, or run and advance to the next cell with Shift-Enter.\n",
    "* Code cells will print their output, including images, below the cell. Running it again deletes the previous output, so be careful if you want to save some results.\n",
    "* You don't have to rerun all cells to test changes, just rerun the cell you have made changes to. Some exceptions might apply, for example if you overwrite variables from previous cells, but in general this will work.\n",
    "* If all else fails, use the \"Kernel\" menu (if you are using Jupyter lab) and select \"Restart Kernel and Clear All Output\". You can also use this menu to run all cells. The same options in VSCode are available in the menu at the top of the notebook.\n",
    "\n",
    "### **0.5 Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically reload modules when changed\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "# Plot figures \"inline\" with other output\n",
    "%matplotlib inline\n",
    "\n",
    "# Import modules, classes, functions\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from utils import loadDataset, splitData, plotTrainingProgress, plotResultsDots, plotConfusionMatrixOCR\n",
    "from evalFunctions import calcAccuracy, calcConfusionMatrix\n",
    "\n",
    "# Configure nice figures\n",
    "plt.rcParams['figure.facecolor']='white'\n",
    "plt.rcParams['figure.figsize']=(8, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***! IMPORTANT NOTE !***\n",
    "\n",
    "Your implementation should only use the `numpy` (`np`) module. The `numpy` module provides all the functionality you need for this assignment and makes it easier debuging your code. No other modules, e.g. `scikit-learn` or `scipy` among others, are allowed and solutions using modules other than `numpy` will be sent for re-submission. You can find everything you need about `numpy` in the official [documentation](https://numpy.org/doc/stable/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Two-layer neural network**\n",
    "\n",
    "You will now implement the same function you did for the single-layer network, but this time for a two-layer network. This means that you will keep track of two weight matrices `W1` and `W2`, two bias vectors `B1` and `B2`. The structure of the code is otherwise still the same, with a `forward`, `backward`, and `update` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1.1 Implementing the forward pass**\n",
    "\n",
    "This implementation will look similar to the single-layer code, but note that you should also return the intemediate variable `U`, which is the output of the hidden layer after passing throught the activation function. This will be used in the backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X, W1, B1, W2, B2, useTanhOutput=False):\n",
    "    \"\"\"Forward pass of two layer network\n",
    "\n",
    "    Args:\n",
    "        X (array): Input samples.\n",
    "        W1 (array): First layer neural network weights.\n",
    "        B1 (array): First layer neural network biases.\n",
    "        W2 (array): Second layer neural network weights.\n",
    "        B2 (array): Second layer neural network biases.\n",
    "\n",
    "    Optional Args:\n",
    "        useTanhOutput (bool) (optional):\n",
    "            True  - Network uses tanh activation on output layer\n",
    "            False - Network uses linear (no) activation on output layer\n",
    "            You are not required to implement this, but it may be useful for faster training.\n",
    "\n",
    "    Returns:\n",
    "        Y (array): Output for each sample and class.\n",
    "        L (array): Resulting label of each sample.\n",
    "        U (array): Output of hidden layer.\n",
    "    \"\"\"\n",
    "\n",
    "    # --------------------------------------------\n",
    "    # === Your code here =========================\n",
    "    # --------------------------------------------\n",
    "    \n",
    "    Y = ???\n",
    "    U = ???\n",
    "    \n",
    "    # ============================================\n",
    "    \n",
    "    # Calculate labels\n",
    "    L = Y.argmax(axis=1)\n",
    "\n",
    "    return Y, L, U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1.2 Implementing the backward pass**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(W1, B1, W2, B2, X, U, Y, D, useTanhOutput=False):\n",
    "    \"\"\"Compute the gradients for network weights and biases\n",
    "\n",
    "    Args:\n",
    "        W1 (array): Current values of the layer 1 network weights.\n",
    "        B1 (array): Current values of the layer 1 network biases.\n",
    "        W2 (array): Current values of the layer 2 network weights.\n",
    "        B2 (array): Current values of the layer 2 network biases.\n",
    "        X (array): Training samples.\n",
    "        U (array): Intermediate outputs of the hidden layer.\n",
    "        Y (array): Predicted outputs.\n",
    "        D (array): Target outputs.\n",
    "        \n",
    "        useTanhOutput (bool): (optional)\n",
    "            True  - Network uses tanh activation on output layer\n",
    "            False - Network uses linear (no) activation on output layer\n",
    "    \n",
    "    Note:\n",
    "        The 'useTanhOutput' parameter only controls the activation on the\n",
    "        output layer, not the hidden layer. The hidden layer must always use\n",
    "        a nonlinear activation function (e.g. tanh) to work properly.\n",
    "        \n",
    "    Returns:\n",
    "        GradW1 (array): Gradients with respect to W1\n",
    "        GradB1 (array): Gradients with respect to B1\n",
    "        GradW2 (array): Gradients with respect to W2\n",
    "        GradB2 (array): Gradients with respect to B2\n",
    "    \"\"\"\n",
    "    \n",
    "    N  = Y.shape[0]\n",
    "    NC = Y.shape[1]\n",
    "    \n",
    "    # --------------------------------------------\n",
    "    # === Your code here =========================\n",
    "    # --------------------------------------------\n",
    "    \n",
    "    # Gradient for the output layer\n",
    "    GradW2 = ???\n",
    "    GradB2 = ???\n",
    "\n",
    "    # And the input layer\n",
    "    GradW1 = ???\n",
    "    GradB1 = ???\n",
    "    \n",
    "    # ============================================\n",
    "    \n",
    "    return GradW1, GradB1, GradW2, GradB2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1.3 Implementing the weight update**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(W1, B1, W2, B2, GradW1, GradB1, GradW2, GradB2, params):\n",
    "    \"\"\"Update weights and biases using computed gradients.\n",
    "\n",
    "    Args:\n",
    "        W1 (array): Current values of the layer 1 network weights.\n",
    "        B1 (array): Current values of the layer 1 network biases.\n",
    "        W2 (array): Current values of the layer 2 network weights.\n",
    "        B2 (array): Current values of the layer 2 network biases.\n",
    "        \n",
    "        GradW1 (array): Gradients with respect to W1.\n",
    "        GradB1 (array): Gradients with respect to B1.\n",
    "        GradW2 (array): Gradients with respect to W2.\n",
    "        GradB2 (array): Gradients with respect to B2.\n",
    "        \n",
    "        params (dict):\n",
    "            - learningRate: Scale factor for update step.\n",
    "            - momentum: Scale factor for momentum update (optional).\n",
    "        \n",
    "    Returns:\n",
    "        W1 (array): Updated layer 1 weights.\n",
    "        B1 (array): Updated layer 1 biases.\n",
    "        W2 (array): Updated layer 2 weights.\n",
    "        B2 (array): Updated layer 2 biases.\n",
    "    \"\"\"\n",
    "    \n",
    "    LR = params[\"learningRate\"]\n",
    "    \n",
    "    # For optional task on momentum\n",
    "    # Uncomment this is you are working on the optional task on momentum.\n",
    "    # M = params.get(\"momentum\", 0.0)\n",
    "    # PrevGradW1 = params.get(\"PrevGradW1\", 0.0)\n",
    "    # PrevGradB1 = params.get(\"PrevGradB1\", 0.0)\n",
    "    # PrevGradW2 = params.get(\"PrevGradW2\", 0.0)\n",
    "    # PrevGradB2 = params.get(\"PrevGradB2\", 0.0)\n",
    "    \n",
    "    # --------------------------------------------\n",
    "    # === Your code here =========================\n",
    "    # --------------------------------------------\n",
    "    \n",
    "    # Update weights\n",
    "    W1 = ???\n",
    "    B1 = ???\n",
    "    W2 = ???\n",
    "    B2 = ???\n",
    "    \n",
    "    # Uncomment this is you are working on the optional task on momentum.\n",
    "    # params[\"PrevGradW1\"] = ???\n",
    "    # params[\"PrevGradB1\"] = ???\n",
    "    # params[\"PrevGradW2\"] = ???\n",
    "    # params[\"PrevGradB2\"] = ???\n",
    "\n",
    "    # ============================================\n",
    "    \n",
    "    return W1, B1, W2, B2\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1.4 Validation on a simple test case**\n",
    "\n",
    "We now perform the same validation with a small test case as in the single-layer network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingStep(X, D, W1, B1, W2, B2, learningRate, useTanhOutput=False):\n",
    "    \"\"\"\n",
    "    Perform one training step of the single layer neural network.\n",
    "\n",
    "    Args:\n",
    "        X (array): Input samples.\n",
    "        D (array): Target outputs.\n",
    "        W (array): Current weights of the network.\n",
    "        B (array): Current biases of the network.\n",
    "        learningRate (float): Learning rate for weight updates.\n",
    "        useTanhOutput (bool) (optional):\n",
    "    \"\"\"\n",
    "\n",
    "    # Perform forward and backward passes, then update weights\n",
    "    Y, L, U = forward(X, W1, B1, W2, B2, useTanhOutput)\n",
    "    dW1, dB1, dW2, dB2 = backward(W1, B1, W2, B2, X, U, Y, D, useTanhOutput)\n",
    "    W1_new, B1_new, W2_new, B2_new = update(W1, B1, W2, B2, dW1, dB1, dW2, dB2, {\"learningRate\": learningRate})\n",
    "\n",
    "    return Y, L, dW1, dB1, dW2, dB2, W1_new, B1_new, W2_new, B2_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import checkExpectedResults\n",
    "\n",
    "# Random test case data and parameters\n",
    "X  = np.array( [[0.38, 0.31], [0.08, 0.15], [0.36, 0.66]] )\n",
    "D  = np.array( [[1, 0], [0, 1], [1, 0]] )\n",
    "W1 = np.array( [[0.82, 0.38, 0.92], [0.57, 0.35, 0.84]] )\n",
    "B1 = np.array( [[0.12, 0.36, 0.27]] )\n",
    "W2 = np.array( [[0.72, 0.72], [0.27, 0.79], [0.52, 0.76]] )\n",
    "B2 = np.array( [[0.57, 0.16]] )\n",
    "learningRate = 0.88\n",
    "\n",
    "\n",
    "# Test case 1: No activation function\n",
    "print(\"------------------------------------------\\nTest case 1: No activation function\\n\")\n",
    "Y, L, dW1, dB1, dW2, dB2, W1_new, B1_new, W2_new, B2_new = trainingStep(X, D, W1, B1, W2, B2, learningRate, useTanhOutput=False)\n",
    "\n",
    "Yexp = np.array( [[1.48, 1.52], [1.1, 1.01], [1.64, 1.75]] )\n",
    "Lexp = np.array( [1, 0, 1] )\n",
    "dW1exp = np.array( [[0.53, 0.48, 0.33], [0.71, 0.64, 0.43]] )\n",
    "dB1exp = np.array( [[1.82, 1.43, 1.15]] )\n",
    "dW2exp = np.array( [[0.65, 1.32], [0.74, 1.28], [0.89, 1.67]] )\n",
    "dB2exp = np.array( [[1.48, 2.19]] )\n",
    "W1_new_exp = np.array( [[0.35, -0.04, 0.63], [-0.06, -0.21, 0.46]] )\n",
    "B1_new_exp = np.array( [[-1.48, -0.89, -0.74]] )\n",
    "W2_new_exp = np.array( [[0.15, -0.44], [-0.38, -0.34], [-0.27, -0.71]] )\n",
    "B2_new_exp = np.array( [[-0.73, -1.76]] )\n",
    "\n",
    "checkExpectedResults(Y, L, [dW1, dW2], [dB1, dB2], [W1_new, W2_new], [B1_new, B2_new], \n",
    "                     Yexp=Yexp, Lexp=Lexp, dWexp=[dW1exp, dW2exp], dBexp=[dB1exp, dB2exp],\n",
    "                     W_new_exp=[W1_new_exp, W2_new_exp], B_new_exp=[B1_new_exp, B2_new_exp],\n",
    "                     decimals=2)\n",
    "\n",
    "# Test case 2: With tanh activation function (same data as test case 1)\n",
    "print(\"\\n----------------------------------------\\nTest case 2: With tanh output activation function\\n\")\n",
    "Y, L, dW1, dB1, dW2, dB2, W1_new, B1_new, W2_new, B2_new = trainingStep(X, D, W1, B1, W2, B2, learningRate, useTanhOutput=True)\n",
    "\n",
    "Yexp = np.array( [[0.9, 0.91], [0.8, 0.77], [0.93, 0.94]] )\n",
    "Lexp = np.array( [1, 0, 1] )\n",
    "dW1exp = np.array( [[0.03, 0.03, 0.02], [0.04, 0.04, 0.03]] )\n",
    "dB1exp = np.array( [[0.16, 0.09, 0.09]] )\n",
    "dW2exp = np.array( [[0.04, 0.09], [0.07, 0.08], [0.07, 0.11]] )\n",
    "dB2exp = np.array( [[0.17, 0.11]] )\n",
    "W1_new_exp = np.array( [[0.79, 0.35, 0.9], [0.53, 0.32, 0.81]] )\n",
    "B1_new_exp = np.array( [[-0.02, 0.28, 0.19]] )\n",
    "W2_new_exp = np.array( [[0.69, 0.64], [0.21, 0.72], [0.46, 0.67]] )\n",
    "B2_new_exp = np.array( [[0.42, 0.06]] )\n",
    "\n",
    "checkExpectedResults(Y, L, [dW1, dW2], [dB1, dB2], [W1_new, W2_new], [B1_new, B2_new], \n",
    "                     Yexp=Yexp, Lexp=Lexp, dWexp=[dW1exp, dW2exp], dBexp=[dB1exp, dB2exp],\n",
    "                     W_new_exp=[W1_new_exp, W2_new_exp], B_new_exp=[B1_new_exp, B2_new_exp],\n",
    "                     decimals=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1.4 The training function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainTwoLayer(XTrain, DTrain, XTest, DTest, W1_0, B1_0, W2_0, B2_0, params):\n",
    "    \"\"\"Trains a two-layer network\n",
    "\n",
    "    Args:\n",
    "        XTrain (array): Training samples.\n",
    "        DTrain (array): Training target outputs.\n",
    "        XTest (array): Test samples.\n",
    "        DTest (array): Test target outputs.\n",
    "\n",
    "        W1_0 (array): Initial values of the first layer network weights.\n",
    "        B1_0 (array): Initial values of the first layer network biases.\n",
    "        W2_0 (array): Initial values of the second layer network weights.\n",
    "        B2_0 (array): Initial values of the second layer network biases.\n",
    "\n",
    "        params (dict): Dictionary containing:\n",
    "            epochs (int): Number of training steps.\n",
    "            learningRate (float): Size of a training step.\n",
    "            useTanhOutput (bool, optional): Determines if output layer should use tanh activation.\n",
    "            momentum (float, optional): Scale factor for momentum update.\n",
    "\n",
    "    Returns:\n",
    "        W1 (array): First layer weights after training.\n",
    "        B1 (array): Fisrt layer biases after training.\n",
    "        W2 (array): Second layer weights after training.\n",
    "        B2 (array): Second layer biases after training.\n",
    "        metrics (dict): Losses and accuracies for training and test data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize variables\n",
    "    metrics = {keys:np.zeros(params[\"epochs\"]+1) for keys in [\"lossTrain\", \"lossTest\", \"accTrain\", \"accTest\"]}\n",
    "\n",
    "    if \"useTanhOutput\" not in params:\n",
    "        params[\"useTanhOutput\"] = False\n",
    "\n",
    "    if \"momentum\" not in params:\n",
    "        params[\"momentum\"] = 0\n",
    "    \n",
    "    nTrain = XTrain.shape[0]\n",
    "    nTest  = XTest.shape[0]\n",
    "    nClasses = DTrain.shape[1]\n",
    "    \n",
    "    # Set initial weights\n",
    "    W1 = W1_0\n",
    "    B1 = B1_0\n",
    "    W2 = W2_0\n",
    "    B2 = B2_0\n",
    "    \n",
    "    # For optional task on momentum\n",
    "    params[\"PrevGradW1\"] = np.zeros_like(W1)\n",
    "    params[\"PrevGradB1\"] = np.zeros_like(B1)\n",
    "    params[\"PrevGradW2\"] = np.zeros_like(W2)\n",
    "    params[\"PrevGradB2\"] = np.zeros_like(B2)\n",
    "\n",
    "    # Get class labels\n",
    "    LTrain = np.argmax(DTrain, axis=1)\n",
    "    LTest  = np.argmax(DTest , axis=1)\n",
    "\n",
    "    # Calculate initial metrics\n",
    "    YTrain, LTrainPred, UTrain = forward(XTrain, W1, B1, W2, B2, params[\"useTanhOutput\"])\n",
    "    YTest , LTestPred , _      = forward(XTest , W1, B1, W2, B2, params[\"useTanhOutput\"])\n",
    "    \n",
    "    # Including the initial metrics makes the progress plots worse, set nan to exclude\n",
    "    metrics[\"lossTrain\"][0] = np.nan # ((YTrain - DTrain)**2).mean()\n",
    "    metrics[\"lossTest\"][0]  = np.nan # ((YTest  - DTest )**2).mean()\n",
    "    metrics[\"accTrain\"][0]  = np.nan # (LTrainPred == LTrain).mean()\n",
    "    metrics[\"accTest\"][0]   = np.nan # (LTestPred  == LTest ).mean()\n",
    "\n",
    "    # Create figure for plotting progress\n",
    "    fig = plt.figure(figsize=(20,8), tight_layout=True)\n",
    "\n",
    "    # Training loop\n",
    "    for n in range(1, params[\"epochs\"]+1):\n",
    "        \n",
    "        # Compute gradients...\n",
    "        GradW1, GradB1, GradW2, GradB2 = backward(W1, B1, W2, B2, XTrain, UTrain, YTrain, DTrain, params[\"useTanhOutput\"])\n",
    "        # ... and update weights\n",
    "        W1, B1, W2, B2 = update(W1, B1, W2, B2, GradW1, GradB1, GradW2, GradB2, params)\n",
    "        \n",
    "        # Evaluate errors\n",
    "        YTrain, LTrainPred, UTrain = forward(XTrain, W1, B1, W2, B2, params[\"useTanhOutput\"])\n",
    "        YTest , LTestPred , _      = forward(XTest , W1, B1, W2, B2, params[\"useTanhOutput\"])\n",
    "        metrics[\"lossTrain\"][n] = ((YTrain - DTrain)**2).mean()\n",
    "        metrics[\"lossTest\"][n]  = ((YTest  - DTest )**2).mean()\n",
    "        metrics[\"accTrain\"][n]  = (LTrainPred == LTrain).mean()\n",
    "        metrics[\"accTest\"][n]   = (LTestPred  == LTest ).mean()\n",
    "\n",
    "        # Plot progress\n",
    "        if (not n % (params[\"epochs\"] // 25)) or (n == params[\"epochs\"]):\n",
    "            plotMode = \"network\" if W1.shape[0] < 64 else \"ocr\"\n",
    "            plotTrainingProgress(fig, [W1, W2], [B1, B2], metrics, n=n, cmap='coolwarm', mode=plotMode)\n",
    "\n",
    "    return W1, B1, W2, B2, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define the same function for normalizing the data, which is even more important now that we have more than one layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(X, XRef):\n",
    "    \"\"\"\n",
    "    Normalizes the data X with the mean and standard deviation of the reference data XRef. These can be the same dataset.\n",
    "    \n",
    "    Args:\n",
    "       X (array): Data matrix to be normalized, features are in axis 0.\n",
    "       XRef (array): Data matrix for calculating the normalization parameters, features are in axis 0.\n",
    "       \n",
    "    Returns:\n",
    "       X (array): Input X normalized with XRef parameters.\n",
    "    \"\"\"\n",
    "    # Compute mean and std of the reference data set\n",
    "    m = XRef.mean(axis=0)\n",
    "    s = XRef.std(axis=0)\n",
    "    # Prevent division by 0 is feature has no variance\n",
    "    s[s == 0] = 1\n",
    "    # Return normalized data\n",
    "    return (X - m) / s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **<span style=\"color:red\">Question 1:</span>**\n",
    "\n",
    "Explain why large, non-normalized input features might be a problem when using two layers, but not when using a single layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **<span style=\"color:green\">Answer:</span>**\n",
    "\n",
    "\\[ Your answers here \\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### **2 Optimizing each dataset**\n",
    "\n",
    "Like before, we define a function that performs the steps for training the networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def  trainTwoLayerOnDataset(datasetNr, testSplit, W1_0, B1_0, W2_0, B2_0, params):\n",
    "    \"\"\"Train a two layer network on a specific dataset.\n",
    "\n",
    "    Ags:\n",
    "        datasetNr (int): ID of dataset to use\n",
    "        testSplit (float): Fraction of data reserved for testing.\n",
    "        W1_0 (array): Initial values of the first layer network weights.\n",
    "        B1_0 (array): Initial values of the first layer network biases.\n",
    "        W2_0 (array): Initial values of the second layer network weights.\n",
    "        B2_0 (array): Initial values of the second layer network biases.\n",
    "        params (dict): Dictionary containing:\n",
    "            nIterations (int): Number of training steps.\n",
    "            learningRate (float): Size of a training step.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load data and split into training and test sets\n",
    "    X, D, L = loadDataset(datasetNr)\n",
    "    XTrain, DTrain, LTrain, XTest, DTest, LTest = splitData(X, D, L, testSplit)\n",
    "    \n",
    "    if \"useTanhOutput\" not in params:\n",
    "        params[\"useTanhOutput\"] = False\n",
    "\n",
    "    if \"normalize\" in params and params[\"normalize\"]:\n",
    "        XTrainNorm = normalize(XTrain, XTrain)\n",
    "        XTestNorm  = normalize(XTest, XTrain)\n",
    "    else:\n",
    "        XTrainNorm = XTrain\n",
    "        XTestNorm  = XTest\n",
    "\n",
    "    # Train network\n",
    "    W1, B1, W2, B2, metrics = trainTwoLayer(XTrainNorm, DTrain, XTestNorm, DTest, W1_0, B1_0, W2_0, B2_0, params)\n",
    "\n",
    "    # Predict classes on test set\n",
    "    LPredTrain = forward(XTrainNorm, W1, B1, W2, B2)[1]\n",
    "    LPredTest  = forward(XTestNorm , W1, B1, W2, B2)[1]\n",
    "\n",
    "    # Compute metrics\n",
    "    accTrain = calcAccuracy(LPredTrain, LTrain)\n",
    "    accTest  = calcAccuracy(LPredTest , LTest)\n",
    "    confMatrix = calcConfusionMatrix(LPredTest, LTest)\n",
    "\n",
    "    # Display results\n",
    "    print(f'Train accuracy: {accTrain:.4f}')\n",
    "    print(f'Test accuracy: {accTest:.4f}')\n",
    "    print(\"Test data confusion matrix:\")\n",
    "    print(confMatrix)\n",
    "\n",
    "    if datasetNr < 4:\n",
    "        plotResultsDots(XTrainNorm, LTrain, LPredTrain, XTestNorm, LTest, LPredTest, lambda X: forward(X, W1, B1, W2, B2, params[\"useTanhOutput\"])[1])\n",
    "    else:\n",
    "        plotConfusionMatrixOCR(XTest, LTest, LPredTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2.1 Optimizing dataset 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# === Your code here =========================\n",
    "# --------------------------------------------\n",
    "\n",
    "nInputs  = ???\n",
    "nClasses = ???\n",
    "nHidden  = ???\n",
    "\n",
    "W1_0 = ???\n",
    "B1_0 = ???\n",
    "W2_0 = ???\n",
    "B2_0 = ???\n",
    "\n",
    "params = {\"epochs\": ???, \"learningRate\": ???, \"normalize\": False, \"useTanhOutput\": False}\n",
    "\n",
    "# ============================================\n",
    "\n",
    "trainTwoLayerOnDataset(1, 0.15, W1_0, B1_0, W2_0, B2_0, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **<span style=\"color:red\">Question 2:</span>**\n",
    "\n",
    "Optimize the model and training hyperparameters until you reach at least 98% test accuracy. Briefly motivate your choice of hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **<span style=\"color:green\">Answer:</span>**\n",
    "\n",
    "\\[ Your answers here \\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2.2 Optimizing dataset 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# === Your code here =========================\n",
    "# --------------------------------------------\n",
    "\n",
    "nInputs  = ???\n",
    "nClasses = ???\n",
    "nHidden  = ???\n",
    "\n",
    "W1_0 = ???\n",
    "B1_0 = ???\n",
    "W2_0 = ???\n",
    "B2_0 = ???\n",
    "\n",
    "params = {\"epochs\": ???, \"learningRate\": ???, \"normalize\": False, \"useTanhOutput\": False}\n",
    "\n",
    "# ============================================\n",
    "\n",
    "trainTwoLayerOnDataset(2, 0.15, W1_0, B1_0, W2_0, B2_0, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **<span style=\"color:red\">Question 3:</span>**\n",
    "\n",
    "Optimize the model and training hyperparameters until you reach at least 99% test accuracy. Briefly motivate your choice of hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **<span style=\"color:green\">Answer:</span>**\n",
    "\n",
    "\\[ Your answers here \\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2.3 Optimizing dataset 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# === Your code here =========================\n",
    "# --------------------------------------------\n",
    "\n",
    "nInputs  = ???\n",
    "nClasses = ???\n",
    "nHidden  = ???\n",
    "\n",
    "W1_0 = ???\n",
    "B1_0 = ???\n",
    "W2_0 = ???\n",
    "B2_0 = ???\n",
    "\n",
    "params = {\"epochs\": ???, \"learningRate\": ???, \"normalize\": False, \"useTanhOutput\": False}\n",
    "\n",
    "# ============================================\n",
    "\n",
    "trainTwoLayerOnDataset(3, 0.15, W1_0, B1_0, W2_0, B2_0, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **<span style=\"color:red\">Question 4:</span>**\n",
    "\n",
    "Optimize the model and training hyperparameters until you reach at least 99% test accuracy. Briefly motivate your choice of hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **<span style=\"color:green\">Answer:</span>**\n",
    "\n",
    "\\[ Your answers here \\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2.4 Optimizing dataset 4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# === Your code here =========================\n",
    "# --------------------------------------------\n",
    "\n",
    "nInputs  = ???\n",
    "nClasses = ???\n",
    "nHidden  = ???\n",
    "\n",
    "W1_0 = ???\n",
    "B1_0 = ???\n",
    "W2_0 = ???\n",
    "B2_0 = ???\n",
    "\n",
    "params = {\"epochs\": ???, \"learningRate\": ???, \"normalize\": False, \"useTanhOutput\": False}\n",
    "\n",
    "# ============================================\n",
    "\n",
    "trainTwoLayerOnDataset(4, 0.15, W1_0, B1_0, W2_0, B2_0, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **<span style=\"color:red\">Question 5:</span>**\n",
    "\n",
    "Optimize the model and training hyperparameters until you reach at least 96% test accuracy. Briefly motivate your choice of hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **<span style=\"color:green\">Answer:</span>**\n",
    "\n",
    "\\[ Your answers here \\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### **3. Optional tasks**\n",
    "\n",
    "Here are some optional tasks that you can try if you are interested to learn more.\n",
    "\n",
    "#### **3.1 Tanh output activations**\n",
    "Implement *tanh* activation in the output layer and re-train the networks in section 2. Do you see any differences in the convergence speed or the decision boundaries?\n",
    "\n",
    "#### **3.2 Momentum gradient descent**\n",
    "So far, you have used normal gradient decent without any modifications. This update rule often suffers from a slow convergence speed, and often gets stuck in local minima. There are many more advanced weight update algorithms that try to fix this problem. The most simple upgrade is to use a momentum term that remembers the previous gradients and uses both those and the new to update the weights. In particular, normal gradient descent is defined as\n",
    "\n",
    "$$ \\large W \\leftarrow W - \\alpha \\nabla W $$\n",
    "\n",
    "with learning rate $\\alpha$. Momentum graient descent is instead defined as\n",
    "\n",
    "$$ \\large G_t = \\beta G_{t-1} + \\alpha \\nabla W $$\n",
    "\n",
    "$$ \\large W \\leftarrow W - G_t $$\n",
    "\n",
    "where $\\beta$ is called the momentum term. A typical value of $\\beta$ is 0.9, which means previous gradients decays exponetially with exponent 0.9. Intuitively, momentum can be though of in the physical sense, where a ball rolling down a hill retains momentum and can therefore get over small bumps in the hill, even if they temporarilly goes upwards. The decay of old gradient can then be thought of as friction. Normal gradient descent has none of these physical intuitions, and will immediately get stuck if it encounters a local minima.\n",
    "\n",
    "Your task is to change the implementation of `update` and rerun the training. A good showcase of momentum should be dataset 4, which should converge much faster when using $\\beta = 0.9$ compared to $\\beta = 0$. If you want to go even further after this, you can take a look at [this](https://towardsdatascience.com/optimizers-for-training-neural-network-59450d71caf6) blog post which neatly summarizes some of the most common update rules even more powerful than momentum.\n",
    "\n",
    "#### **3.3 Hyperparameter search**\n",
    "\n",
    "Manually optimizing the hyperparameters can get tedious, so why not do it automatically? If you have no problems with your computer becomming a heat radiator, an interesting experiment is to implement a grid search over some hyperparameters, for example learning rate, momentum, and size of the hidden layer, and see where the optimal combination is. You probably have to modify the `trainTwoLayerOnDataset` function in two ways to do this.\n",
    "1. Return the metric you want to optimize, for example the final test loss.\n",
    "2. Provide a `seed` input to the `splitData` function, to make sure the grid search always uses the same data for each combination of hyper parameters.\n",
    "\n",
    "#### **3.4 L2 regularization**\n",
    "\n",
    "You can also try to implement so called L2 regularization, which adds a penalty term to the loss function that penalizes large weights. This is done by adding the term\n",
    "\n",
    "$$ L_2 = \\frac{\\lambda}{2} \\left( \\sum W^2 + \\sum V^2 \\right)$$\n",
    "\n",
    "to the loss function, i.e. the scaled sum of all weights, where $\\lambda$ is a hyperparameter that controls how strong the penalty is. This will in turn add the term\n",
    "\n",
    "$$ \\nabla L_2 = \\large \\lambda \\left( W + V \\right) $$\n",
    "\n",
    "to the gradient when performing backpropagation. Implement this and see if it improves the performance, especially on dataset 4 which is prone to overfitting due to the large number of parameters.\n",
    "\n",
    "#### **3.5 Multi-layer network**\n",
    "\n",
    "As a final optional task, you can try to extend your implementation to support more than two layers. This will require a more general implementation of the forward and backward passes, as well as the weight update, using loops and iterative updates of the gradients as you progress through the layers. The skeleton code is provided in the `MultiLayer.ipynb` notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TBMI26_New",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
