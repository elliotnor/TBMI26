{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add your LiU-ID here:**\n",
    "* <liuid 1>\n",
    "* <liuid 2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **0. Quick introduction to jupyter notebooks**\n",
    "* Each cell in this notebook contains either code or text.\n",
    "* You can run a cell by pressing Ctrl-Enter, or run and advance to the next cell with Shift-Enter.\n",
    "* Code cells will print their output, including images, below the cell. Running it again deletes the previous output, so be careful if you want to save some results.\n",
    "* You don't have to rerun all cells to test changes, just rerun the cell you have made changes to. Some exceptions might apply, for example if you overwrite variables from previous cells, but in general this will work.\n",
    "* If all else fails, use the \"Kernel\" menu (if you are using Jupyter lab) and select \"Restart Kernel and Clear All Output\". You can also use this menu to run all cells. The same options in VSCode are available in the menu at the top of the notebook.\n",
    "\n",
    "### **0.5 Setup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically reload modules when changed\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "# Plot figures \"inline\" with other output\n",
    "%matplotlib inline\n",
    "\n",
    "# Import modules, classes, functions\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from utils import loadDataset, splitData, plotTrainingProgress, plotResultsDots, plotConfusionMatrixOCR\n",
    "from evalFunctions import calcAccuracy, calcConfusionMatrix\n",
    "\n",
    "# Configure nice figures\n",
    "plt.rcParams['figure.facecolor']='white'\n",
    "plt.rcParams['figure.figsize']=(8, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***! IMPORTANT NOTE !***\n",
    "\n",
    "Your implementation should only use the `numpy` (`np`) module. The `numpy` module provides all the functionality you need for this assignment and makes it easier debuging your code. No other modules, e.g. `scikit-learn` or `scipy` among others, are allowed and solutions using modules other than `numpy` will be sent for re-submission. You can find everything you need about `numpy` in the official [documentation](https://numpy.org/doc/stable/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### **1. Single layer neural network**\n",
    "\n",
    "In this and the next notebook, you will implement and train two types of neural networks. We begin with a single layer network in this notebook, where the predicted classes are based on linear combinations of the input features. The single layer network can only learn decision boundaries. The next notebook introduces the multi-layer network, which is a general function approximator and therefore can learn any function given enough resourses.\n",
    "\n",
    "Training a network consist of three main steps:\n",
    "1. Forward pass - Comupte predicted outputs based on inputs features.\n",
    "2. Backward pass - Compute weight gradients.\n",
    "3. Update - Use gradients and hyperparameters to update weights.\n",
    "\n",
    "Begin by implementing these three functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1.1 Implementing the forward pass**\n",
    "\n",
    "In the following function you will implement the forward pass, i.e. take the input features `X`, weights `W`, and biases `B`, and compute the predicted outputs of the network. Optionally, you can choose to also implement the forward pass when using *tanh* activation function in the output layer. This does not make the classifier non-linear, but it can help speed up the training.\n",
    "\n",
    "Note: In `numpy`, the multiplication symbol `*` means element-wise multiplication. Matrix multiplication is done using the `@` symbol, for example `A @ B` where `A` and `B` are compatible matrices. Transposing a `numpy` matrix is done with `A.T`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(X, W, B, useTanhOutput=False):\n",
    "    \"\"\"Forward pass of single layer network.\n",
    "\n",
    "    Performs one forward pass of the single layer network, i.e\n",
    "    it takes the input data and calculates the output for each sample.\n",
    "\n",
    "    Args:\n",
    "        X (array): Input samples.\n",
    "        W (array): Neural network weights.\n",
    "        B (array): Neural network biases.\n",
    "\n",
    "    Optional Args:\n",
    "        useTanhOutput (bool) (optional):\n",
    "            True  - Network uses tanh activation on output layer\n",
    "            False - Network uses linear (no) activation on output layer\n",
    "            You are not required to implement this, but it may be useful for faster training.\n",
    "\n",
    "    Returns:\n",
    "        Y (array): Output for each sample and class.\n",
    "        L (array): Resulting label of each sample.\n",
    "    \"\"\"\n",
    "    \n",
    "    # --------------------------------------------\n",
    "    # === Your code here =========================\n",
    "    # --------------------------------------------\n",
    "    \n",
    "    Y = ???\n",
    "        \n",
    "    # ============================================\n",
    "        \n",
    "    # Calculate labels\n",
    "    L = np.argmax(Y, axis=1)\n",
    "    \n",
    "    return Y, L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1.2 Implementing the backward pass**\n",
    "\n",
    "Now, using the predicted outputs `Y` from the forward pass, compute the gradients of the weights and biases. To do this you must use the target outputs `D`. If you print `D` you will see that it contains values that are ±0.99. This might seem strange at first, where ±1 would be a more intuitive choice. However, this is a trick to prevent the weights of the network from becomming too large when using the optional *tanh* activation in the output layer. Since *tanh* never reach ±1 (only in the limit at ±Inf) it would push the weight to very large values in order to reach closer and closer to those limits. This is undesirable, so instead we choose the targets ±0.99, which only require the inputs to *tanh* to be approximately ±2.6, allowing much smaller weights and therefore a more stable classifier. None of this should matter for your implementation of the backward pass, but it is nonetheless good to understand the data you are working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(W, B, X, Y, D, useTanhOutput=False):\n",
    "    \"\"\"Compute the gradients for network weights and biases\n",
    "\n",
    "    Args:\n",
    "        W (array): Current values of the network weights.\n",
    "        B (array): Current values of the network biases.\n",
    "        X (array): Training samples.\n",
    "        Y (array): Predicted outputs.\n",
    "        D (array): Target outputs.\n",
    "        \n",
    "        useTanhOutput (bool) (optional):\n",
    "            True  - Network uses tanh activation on output layer\n",
    "            False - Network uses linear (no) activation on output layer\n",
    "        \n",
    "    Returns:\n",
    "        GradW (array): Gradients with respect to W\n",
    "        GradB (array): Gradients with respect to B\n",
    "    \"\"\"\n",
    "    \n",
    "    N = Y.shape[0]\n",
    "    \n",
    "    # --------------------------------------------\n",
    "    # === Your code here =========================\n",
    "    # --------------------------------------------\n",
    "    \n",
    "    # Calculate gradients\n",
    "    GradW = ???\n",
    "    GradB = ???\n",
    "    \n",
    "    # ============================================\n",
    "    \n",
    "    return GradW, GradB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1.3 Implementing the weight update**\n",
    "\n",
    "Finally, after computing the gradients using the `backward` function, update and return the new weights and biases. While there are many advanced updated methods, we will only look at the most basic in this assignment,  unmodified gradient descent. Implement the following function, which should be very short."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(W, B, GradW, GradB, params):\n",
    "    \"\"\"Update weights and biases using computed gradients.\n",
    "\n",
    "    Args:\n",
    "        W (array): Current values of the network weights.\n",
    "        B (array): Current values of the network biases.\n",
    "        GradW (array): Gradients with respect to W.\n",
    "        GradB (array): Gradients with respect to B.\n",
    "        \n",
    "        params (dict):\n",
    "            - learningRate: Scale factor for update step.\n",
    "        \n",
    "    Returns:\n",
    "        W (array): Updated weights.\n",
    "        B (array): Updated biases.\n",
    "    \"\"\"\n",
    "    \n",
    "    LR = params[\"learningRate\"]\n",
    "    \n",
    "    # --------------------------------------------\n",
    "    # === Your code here =========================\n",
    "    # --------------------------------------------\n",
    "    \n",
    "    W = ???\n",
    "    B = ???\n",
    "    \n",
    "    # ============================================\n",
    "    \n",
    "    return W, B\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1.4 Validation on a simple test case**\n",
    "\n",
    "To help you verify that your implementations of the above three functions are correct, we have defined a very simple test case with known outputs. After implementing the three functions, run the following cells to verify that your implementations are correct. If everything is correct, you should not see any assertion errors. Note that the test code performs the comparission both *without* and *with* tanh output activation. You can disregard the second case if you have not implemneted this optional task yet.\n",
    "\n",
    "To simplify the tests, we define a ``trainingStep`` function that combines all three above steps into one for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingStep(X, D, W, B, learningRate, useTanhOutput=False):\n",
    "    \"\"\"\n",
    "    Perform one training step of the single layer neural network.\n",
    "\n",
    "    Args:\n",
    "        X (array): Input samples.\n",
    "        D (array): Target outputs.\n",
    "        W (array): Current weights of the network.\n",
    "        B (array): Current biases of the network.\n",
    "        learningRate (float): Learning rate for weight updates.\n",
    "        useTanhOutput (bool) (optional):\n",
    "    \"\"\"\n",
    "\n",
    "    # Perform forward and backward passes, then update weights\n",
    "    Y, L = forward(X, W, B, useTanhOutput)\n",
    "    dW, dB = backward(W, B, X, Y, D, useTanhOutput)\n",
    "    W_new, B_new = update(W, B, dW, dB, {\"learningRate\": learningRate})\n",
    "\n",
    "    return Y, L, dW, dB, W_new, B_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we run the test case:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import checkExpectedResults\n",
    "\n",
    "# Random test case data and parameters\n",
    "X = np.array( [[0.92, 0.86], [0.68, 0.18], [0.99, 0.0]] )\n",
    "D = np.array( [[1, 0], [0, 1], [1, 0]] )\n",
    "W = np.array( [[0.44, 0.38], [0.07, 0.69]] )\n",
    "B = np.array( [[0.72, 0.66]] )\n",
    "learningRate = 1.29\n",
    "\n",
    "# Test case 1: No activation function\n",
    "print(\"------------------------------------------\\nTest case 1: No activation function\\n\")\n",
    "Y, L, dW, dB, W_new, B_new = trainingStep(X, D, W, B, learningRate, useTanhOutput=False)\n",
    "\n",
    "Yexp = np.array( [[1.18, 1.6], [1.03, 1.04], [1.16, 1.04]] )\n",
    "Lexp = np.array( [1, 1, 0] )\n",
    "dWexp = np.array( [[0.68, 1.69], [0.23, 0.92]] )\n",
    "dBexp = np.array( [[0.91, 1.79]] )\n",
    "W_new_exp = np.array( [[-0.44, -1.8], [-0.23, -0.5]] )\n",
    "B_new_exp = np.array( [[-0.46, -1.65]] )\n",
    "\n",
    "checkExpectedResults(Y, L, dW, dB, W_new, B_new, \n",
    "                     Yexp=Yexp, Lexp=Lexp, dWexp=dWexp, dBexp=dBexp,\n",
    "                     W_new_exp=W_new_exp, B_new_exp=B_new_exp,\n",
    "                     decimals=2)\n",
    "\n",
    "# Test case 2: With tanh activation function (same data as test case 1)\n",
    "print(\"\\n----------------------------------------\\nTest case 2: With tanh output activation function\\n\")\n",
    "Y, L, dW, dB, W_new, B_new = trainingStep(X, D, W, B, learningRate, useTanhOutput=True)\n",
    "\n",
    "Yexp = np.array( [[0.83, 0.92], [0.77, 0.78], [0.82, 0.78]] )\n",
    "Lexp = np.array( [1, 1, 0] )\n",
    "dWexp = np.array( [[0.07, 0.25], [0.01, 0.07]] )\n",
    "dBexp = np.array( [[0.13, 0.24]] )\n",
    "W_new_exp = np.array( [[0.35, 0.06], [0.06, 0.6]] )\n",
    "B_new_exp = np.array( [[0.55, 0.35]] )\n",
    "\n",
    "checkExpectedResults(Y, L, dW, dB, W_new, B_new, \n",
    "                     Yexp=Yexp, Lexp=Lexp, dWexp=dWexp, dBexp=dBexp,\n",
    "                     W_new_exp=W_new_exp, B_new_exp=B_new_exp,\n",
    "                     decimals=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1.5 The training function**\n",
    "\n",
    "In order to train the network using your implementation of `forward`, `backward`, and `update`, we have prepared the following function for you. The core of this function is the same as the training step function you used in the last section, except we now also set up all required variables and train the network for the specified number of epochs while tracking metrics and plotting the training progress. Read the function and make sure you understand how your own implementations are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainSingleLayer(XTrain, DTrain, XTest, DTest, W0, B0, params, plotProgress=True):\n",
    "    \"\"\"Trains a single-layer network.\n",
    "\n",
    "    Args:\n",
    "        XTrain (array): Training samples.\n",
    "        DTrain (array): Training target outputs.\n",
    "        XTest (array): Test samples.\n",
    "        DTest (array): Test target outputs.\n",
    "\n",
    "        W0 (array): Initial values of the network weights.\n",
    "        B0 (array): Initial values of the network biases.\n",
    "        \n",
    "        params (dict): Dictionary containing:\n",
    "            epochs (int): Number of training steps.\n",
    "            learningRate (float): Size of a training step.\n",
    "            useTanhOutput (bool, optional): Determines if output layer should use tanh activation.\n",
    "            momentum (float, optional): Scale factor for momentum update.\n",
    "\n",
    "    Returns:\n",
    "        W (array): Weights after training.\n",
    "        B (array): Biases after training.\n",
    "        metrics (dict): Losses and accuracies for training and test data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize variables\n",
    "    metrics = {keys:np.zeros(params[\"epochs\"]+1) for keys in [\"lossTrain\", \"lossTest\", \"accTrain\", \"accTest\"]}\n",
    "    \n",
    "    if \"useTanhOutput\" not in params:\n",
    "        params[\"useTanhOutput\"] = False\n",
    "\n",
    "    nTrain = XTrain.shape[0]\n",
    "    nTest  = XTest.shape[0]\n",
    "\n",
    "    # Set initial weights\n",
    "    W = W0\n",
    "    B = B0\n",
    "\n",
    "    # Get class labels\n",
    "    LTrain = np.argmax(DTrain, axis=1)\n",
    "    LTest  = np.argmax(DTest , axis=1)\n",
    "\n",
    "    # Calculate initial metrics\n",
    "    YTrain, LTrainPred = forward(XTrain, W, B, params[\"useTanhOutput\"])\n",
    "    YTest , LTestPred  = forward(XTest , W, B, params[\"useTanhOutput\"])\n",
    "    \n",
    "    # Including the initial metrics makes the progress plots worse, set nan to exclude\n",
    "    metrics[\"lossTrain\"][0] = np.nan #((YTrain - DTrain)**2).mean()\n",
    "    metrics[\"lossTest\"][0]  = np.nan #((YTest  - DTest )**2).mean()\n",
    "    metrics[\"accTrain\"][0]  = np.nan #(LTrainPred == LTrain).mean()\n",
    "    metrics[\"accTest\"][0]   = np.nan #(LTestPred  == LTest ).mean()\n",
    "\n",
    "    # Create figure for plotting progress\n",
    "    if W0.shape[0] < 64:\n",
    "        fig = plt.figure(figsize=(20,8), tight_layout=True)\n",
    "    else:\n",
    "        fig = plt.figure(figsize=(20,8), constrained_layout=True)\n",
    "\n",
    "    # Training loop\n",
    "    for n in range(1, params[\"epochs\"]+1):\n",
    "        \n",
    "        # Compute gradients...\n",
    "        GradW, GradB = backward(W, B, XTrain, YTrain, DTrain, params[\"useTanhOutput\"])\n",
    "        # ... and update weights\n",
    "        W, B = update(W, B, GradW, GradB, params)\n",
    "        \n",
    "        # Evaluate errors\n",
    "        YTrain, LTrainPred = forward(XTrain, W, B, params[\"useTanhOutput\"])\n",
    "        YTest , LTestPred  = forward(XTest , W, B, params[\"useTanhOutput\"])\n",
    "        metrics[\"lossTrain\"][n] = ((YTrain - DTrain)**2).mean()\n",
    "        metrics[\"lossTest\"][n]  = ((YTest  - DTest )**2).mean()\n",
    "        metrics[\"accTrain\"][n]  = (LTrainPred == LTrain).mean()\n",
    "        metrics[\"accTest\"][n]   = (LTestPred  == LTest ).mean()\n",
    "\n",
    "        # Plot progress\n",
    "        if (plotProgress and not n % (params[\"epochs\"] // 25)) or (n == params[\"epochs\"]):\n",
    "            plotMode = \"network\" if W0.shape[0] < 64 else \"ocr\"\n",
    "            plotTrainingProgress(fig, W, B, metrics, n=n, cmap='coolwarm', mode=plotMode)\n",
    "\n",
    "    return W, B, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1.6. Test your implementation**\n",
    "\n",
    "It is time to test your implementation by training the a network on the first dataset. You will begin by running all the required code manually, so you will see and understand each part of the process. Later, we will define a function that does all these steps for you. But for now, run the following cell to load the data and split it into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select and load dataset\n",
    "datasetNr = 1\n",
    "X, D, L = loadDataset(datasetNr)\n",
    "\n",
    "# Split data into training and test sets\n",
    "XTrain, DTrain, LTrain, XTest, DTest, LTest = splitData(X, D, L, 0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One important aspect of data preprocessing is data normalization. Often, normalizing the data can turn a very difficult dataset into a simple one, simply by ensuring that the data is similarly distributed in each feature dimension. This becomes much more important as we increase the complexity of the data, so you might not see much difference in the first three datasets. However, in the OCR data this will be much more important, and even more so in the next assignement on deep learning.\n",
    "\n",
    "For now, we define a function that normalizes each feature to have zero mean and unit standard deviation\n",
    "\n",
    "$$ \\large m = \\frac{1}{N} \\sum_i^N X_i \\quad \\quad \\quad \\large s = \\sqrt{ \\frac{1}{N} \\sum_i^N \\left( X_i - m \\right)^2 }$$\n",
    "\n",
    "$$ \\large X_\\mathrm{norm} = \\frac{X - m}{s}$$\n",
    "\n",
    "An important detail that is easy to overlook when normlizing is that we need to use the mean and standard deviation of the training data when normalizing the unseen test data. This ensures that the nomalization is equal between training and test. Furthermore, there are common situations where it is simply not possible to compute a representative mean and standard deviation on the test data, for example if we have a testset with only one sample. Obviously we want to the model to work even if we only input a single sample, and we therefore use the reference values from the training set instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(X, XRef):\n",
    "    \"\"\"\n",
    "    Normalizes the data X with the mean and standard deviation of the reference data XRef. These can be the same dataset.\n",
    "    \n",
    "    Args:\n",
    "       X (array): Data matrix to be normalized, features are in axis 0.\n",
    "       XRef (array): Data matrix for calculating the normalization parameters, features are in axis 0.\n",
    "       \n",
    "    Returns:\n",
    "       X (array): Input X normalized with XRef parameters.\n",
    "    \"\"\"\n",
    "    # Compute mean and std of the reference data set\n",
    "    m = XRef.mean(axis=0)\n",
    "    s = XRef.std(axis=0)\n",
    "    # Prevent division by 0 is feature has no variance\n",
    "    s[s == 0] = 1\n",
    "    # Return normalized data\n",
    "    return (X - m) / s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this code to normalize the training and test data. These are the datasets you will input to the training function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XTrainNorm = normalize(XTrain, XTrain)\n",
    "XTestNorm  = normalize(XTest, XTrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should initialize your weights and biases. It is up to you to figure out the correct shapes of `W0` and `B0` based on the number of inputs and number of output classes. You also need to experiment with different initialization strategies. For example, should the weights be all zeros, all ones, random, and in that case what distribution and magnitude? The only requirement we give you is that `W0` and `B0` are numpy arrays, to make the rest of the code work as expected.\n",
    "\n",
    "*Hint: The question how to initialize the network parameters is not new, and many clever researchers have tried to come up with automatic solutions. If you are interested you can search for \"Xavier initialization\" and try to implement it.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# === Your code here ========================= \n",
    "# --------------------------------------------\n",
    "\n",
    "nInputs  = ???\n",
    "nClasses = ???\n",
    "\n",
    "W0 = ???\n",
    "B0 = ???\n",
    "\n",
    "# ============================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now set the training parameters in the `params` dictionary. Later, you will add some more parameters, but for this first test the number of `epochs` and the `learningRate` is sufficient. Then run the training and observe how the losses, accuracies, and network weights change as the training progresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\"epochs\": 1000, \"learningRate\": 0.05}\n",
    "W, B, metrics = trainSingleLayer(XTrainNorm, DTrain, XTestNorm, DTest, W0, B0, params, plotProgress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use the trained weights to predict the output labels. To investigate overfitting, we predict both the training and test labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LPredTrain = forward(XTrainNorm, W, B)[1]\n",
    "LPredTest  = forward(XTestNorm , W, B)[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the labels to get accuracies and the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the training and test accuracy\n",
    "accTrain = calcAccuracy(LPredTrain, LTrain)\n",
    "accTest = calcAccuracy(LPredTest, LTest)\n",
    "print(f'Train accuracy: {accTrain:.4f}')\n",
    "print(f'Test accuracy: {accTest:.4f}')\n",
    "\n",
    "# Calculate confunsion matrix of test data\n",
    "confMatrix = calcConfusionMatrix(LPredTest, LTest)\n",
    "print(\"Test data confusion matrix:\")\n",
    "print(confMatrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, plot the results. For the point-cloud datasets we use the function `plotResultDots`, which shows the predicted class boundaries and predicted class labels. For the OCR dataset we instead use a confusion matrix to visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if datasetNr < 4:\n",
    "    plotResultsDots(XTrainNorm, LTrain, LPredTrain, XTestNorm, LTest, LPredTest, lambda X: forward(X, W, B)[1])\n",
    "else:\n",
    "    plotConfusionMatrixOCR(XTest, LTest, LPredTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### **2 Optimizing each dataset**\n",
    "\n",
    "The process above is a bit long, since you have to load and normalize data, initailize weights, train the network, measure metrics, and plot the results for each dataset. We can make things a bit easier by defining a function that does most of this for us. In addition to the previous parameters `epochs` and `learningRate`, this function also accepts the parameter `normalize` (True / False) which enables or disables the data normalization. You can use this to help answer some questions below. You can also set the optional parameter `useTanhOutput` (True / False) to enable or disable the use of *tanh* activation in the output layer, if you have implemented it in the previous functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainSingleLayerOnDataset(datasetNr, testSplit, W0, B0, params):\n",
    "    \"\"\"Train a single layer network on a specific dataset.\n",
    "\n",
    "    Ags:\n",
    "        datasetNr (int): ID of dataset to use\n",
    "        testSplit (float): Fraction of data reserved for testing.\n",
    "        W0 (array): Initial values of the network weights.\n",
    "        B0 (array): Initial values of the network biases.\n",
    "        \n",
    "        params (dict): Dictionary containing:\n",
    "            epochs (int): Number of training steps.\n",
    "            learningRate (float): Size of a training step.\n",
    "            normalize (bool): Should data be normalized?\n",
    "            useTanhOutput (bool): Should tanh activation be used for outputs?\n",
    "    \"\"\"\n",
    "\n",
    "    # Load data and split into training and test sets\n",
    "    X, D, L = loadDataset(datasetNr)\n",
    "    D = np.round(D)\n",
    "    XTrain, DTrain, LTrain, XTest, DTest, LTest = splitData(X, D, L, testSplit)\n",
    "\n",
    "    if \"useTanhOutput\" not in params:\n",
    "        params[\"useTanhOutput\"] = False\n",
    "\n",
    "    if \"normalize\" in params and params[\"normalize\"]:\n",
    "        XTrainNorm = normalize(XTrain, XTrain)\n",
    "        XTestNorm  = normalize(XTest , XTrain)\n",
    "    else:\n",
    "        XTrainNorm = XTrain\n",
    "        XTestNorm  = XTest\n",
    "    \n",
    "    # Train network\n",
    "    W, B, metrics = trainSingleLayer(XTrainNorm, DTrain, XTestNorm, DTest, W0, B0, params, plotProgress=True)\n",
    "\n",
    "    # Predict classes on test set\n",
    "    LPredTrain = forward(XTrainNorm, W, B)[1]\n",
    "    LPredTest = forward(XTestNorm, W, B)[1]\n",
    "    \n",
    "    # Compute metrics\n",
    "    accTrain = calcAccuracy(LPredTrain, LTrain)\n",
    "    accTest = calcAccuracy(LPredTest, LTest)\n",
    "    confMatrix = calcConfusionMatrix(LPredTest, LTest)\n",
    "    \n",
    "    # Display results\n",
    "    print(f'Train accuracy: {accTrain:.4f}')\n",
    "    print(f'Test accuracy: {accTest:.4f}')\n",
    "    print(\"Test data confusion matrix:\")\n",
    "    print(confMatrix)\n",
    "\n",
    "    if datasetNr < 4:\n",
    "        plotResultsDots(XTrainNorm, LTrain, LPredTrain, XTestNorm, LTest, LPredTest, lambda X: forward(X, W, B, params[\"useTanhOutput\"])[1])\n",
    "    else:\n",
    "        plotConfusionMatrixOCR(XTest, LTest, LPredTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2.1 Optimizing dataset 1**\n",
    "\n",
    "The first dataset is very simple and you should not have any trouble reaching convergence in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# === Your code here ========================= \n",
    "# --------------------------------------------\n",
    "\n",
    "nInputs  = ???\n",
    "nClasses = ???\n",
    "\n",
    "W0 = ???\n",
    "B0 = ???\n",
    "\n",
    "params = {\"epochs\": ???, \"learningRate\": ???, \"normalize\": False, \"useTanhOutput\": False}\n",
    "\n",
    "# ============================================\n",
    "\n",
    "trainSingleLayerOnDataset(1, 0.15, W0, B0, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **<span style=\"color:red\">Question 1:</span>**\n",
    "\n",
    "Optimize the training until you reach at least 98% test accuracy. Briefly motivate your choice of hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **<span style=\"color:green\">Answer:</span>**\n",
    "\n",
    "\\[ Your answers here \\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **<span style=\"color:red\">Question 2:</span>**\n",
    "\n",
    "Explain what effect the bias weight has on the decision boundary in a single-layer network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **<span style=\"color:green\">Answer:</span>**\n",
    "\n",
    "\\[ Your answers here \\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2.2 Optimizing dataset 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# === Your code here ========================= \n",
    "# --------------------------------------------\n",
    "\n",
    "nInputs  = ???\n",
    "nClasses = ???\n",
    "\n",
    "W0 = ???\n",
    "B0 = ???\n",
    "\n",
    "params = {\"epochs\": ???, \"learningRate\": ???, \"normalize\": False, \"useTanhOutput\": False}\n",
    "\n",
    "# ============================================\n",
    "\n",
    "trainSingleLayerOnDataset(2, 0.15, W0, B0, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2.3 Optimizing dataset 3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# === Your code here ========================= \n",
    "# --------------------------------------------\n",
    "\n",
    "nInputs  = ???\n",
    "nClasses = ???\n",
    "\n",
    "W0 = ???\n",
    "B0 = ???\n",
    "\n",
    "params = {\"epochs\": ???, \"learningRate\": ???, \"normalize\": False, \"useTanhOutput\": False}\n",
    "\n",
    "# ============================================\n",
    "\n",
    "trainSingleLayerOnDataset(3, 0.15, W0, B0, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **2.4 Optimizing dataset 4**\n",
    "\n",
    "Dataset 4 contains OCR-data and is therefore more complicated than the previous three. We therefore recommend that you enable normalization to make the problem a bit easier to solve. We also change the network visualization. Since each weight now corresponds to a pixel in the 8x8 images, we can plot the weights connected to each output class as images as well, which in some sense shows which areas in the images are important to recognize different numbers. It might be a bit difficult to see at first, but somtimes it is possible to see hints of the numbers in these weight illustrations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------\n",
    "# === Your code here ========================= \n",
    "# --------------------------------------------\n",
    "\n",
    "nInputs  = ???\n",
    "nClasses = ???\n",
    "\n",
    "W0 = ???\n",
    "B0 = ???\n",
    "\n",
    "params = {\"epochs\": ???, \"learningRate\": ???, \"normalize\": False, \"useTanhOutput\": False}\n",
    "\n",
    "# ============================================\n",
    "\n",
    "trainSingleLayerOnDataset(4, 0.15, W0, B0, params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **<span style=\"color:red\">Question 3:</span>**\n",
    "\n",
    "You should be able to get surprisingly high accuracy on this dataset using the single-layer network. Explain how this is possible.\n",
    "\n",
    "*Hint: Think of the number of input and output dimensions of the problem.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **<span style=\"color:green\">Answer:</span>**\n",
    "\n",
    "\\[ Your answers here \\]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### **3. Optional tasks**\n",
    "Here is an optional task that you can try if you are interested to learn more.\n",
    "\n",
    "#### **3.1 Tanh output activations**\n",
    "Implement *tanh* activation in the output layer and re-train the networks in section 2. Do you see any differences in the convergence speed or the decision boundaries?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TBMI26_New",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
